{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6239e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trace_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8c2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "MTU = 1514\n",
    "MB = 1024 * 1024\n",
    "KB = 1024\n",
    "GB = 1024 * 1024 * 1024\n",
    "SECONDS = 1e9\n",
    "MILLI_SECONDS = 1e6\n",
    "MICRO_SECONDS = 1e3\n",
    "NANO_SECONDS = 1\n",
    "IPS = [212, 237]\n",
    "MASTER = 212\n",
    "TARGET_IP = 237\n",
    "\n",
    "PACKETS = \"../data/40/node-3/train/packets\"\n",
    "ALLOCS = \"../data/40/node-3/train/allocations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a897ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 200\n",
    "TIMERES = 'Milliseconds'\n",
    "TIMERESL = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d8774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_packets(file_path, timefactor=SECONDS, volumefactor=KB):\n",
    "    df = pd.read_csv(\n",
    "        file_path, \n",
    "        header=None,\n",
    "        index_col=False,\n",
    "        names=['timestamp', 'size', 'src_ip', 'dest_ip', 'src_port', 'dest_port', 'dir'], \n",
    "        dtype={\"dir\": \"int8\"},\n",
    "    )\n",
    "    # df = df[df['dir'] == 1]\n",
    "    #df['timestamp'] = (df['timestamp'] - df['timestamp'].min()) / timefactor\n",
    "    df['size'] = df['size'] / volumefactor\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83686952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace(file_path, timefactor=SECONDS, volumefactor=KB):\n",
    "    df = pd.read_csv(\n",
    "        file_path, \n",
    "        header=None,\n",
    "        index_col=False,\n",
    "        names=['timestamp', 'size'],\n",
    "        sep='\\t'\n",
    "    )\n",
    "    #df['timestamp'] = (df['timestamp'] - df['timestamp'].min()) / timefactor\n",
    "    df['size'] = df['size'] / volumefactor\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2007c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_packets(PACKETS, volumefactor=KB, timefactor=MILLI_SECONDS)\n",
    "df = df[['timestamp', 'size', 'src_ip', 'dest_ip']]\n",
    "df = df[(df['src_ip'] == MASTER) & (df['dest_ip'] == TARGET_IP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e566b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alloc = get_trace(ALLOCS, volumefactor=KB, timefactor=MILLI_SECONDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d08238",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m end \u001b[38;5;241m<\u001b[39m ts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     10\u001b[0m     in_ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere((start \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m ts) \u001b[38;5;241m&\u001b[39m (ts \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end))\n\u001b[0;32m---> 11\u001b[0m     in_ts_alloc \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mts_alloc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_alloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x\u001b[38;5;241m.\u001b[39mappend(start)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(in_ts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:2\u001b[0m, in \u001b[0;36margwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ts = df['timestamp'].values\n",
    "vals = df['size'].values\n",
    "ts_alloc = df_alloc['timestamp'].values\n",
    "vals_alloc = df_alloc['size'].values\n",
    "y = []\n",
    "ya = []\n",
    "x = []\n",
    "start, end = ts[0], ts[0] + RES\n",
    "while end < ts[-1]:\n",
    "    in_ts = np.argwhere((start <= ts) & (ts <= end))\n",
    "    in_ts_alloc = np.argwhere((start <= ts_alloc) & (ts_alloc <= end))\n",
    "    x.append(start)\n",
    "    if len(in_ts) > 1:\n",
    "        new_s, new_end = in_ts[0][0], in_ts[-1][0]\n",
    "        y.append(np.sum(vals[new_s:new_end]))\n",
    "        x.append(start)\n",
    "        ts = ts[in_ts[-1][0]:]\n",
    "        vals = vals[in_ts[-1][0]:]\n",
    "    else:\n",
    "        y.append(0)\n",
    "    if len(in_ts_alloc) > 1:\n",
    "        new_s_alloc, new_end_alloc = in_ts_alloc[0][0], in_ts_alloc[-1][0]\n",
    "        ya.append(np.sum(vals_alloc[new_s:new_end]))\n",
    "        ts_alloc = ts_alloc[in_ts[-1][0]:]\n",
    "        vals_alloc = vals_alloc[in_ts[-1][0]:]\n",
    "    else:\n",
    "        ya.append(0)\n",
    "        \n",
    "    start, end = start + RES, end + RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8593b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 10))\n",
    "plt.title(f\"Res: {RES} {TIMERES} {MASTER} -> {TARGET_IP}\")\n",
    "plt.xlabel(f\"Time ({TIMERESL})\")\n",
    "plt.ylabel(\"Data, KB\")\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 10))\n",
    "plt.title(f\"Res: {RES} {TIMERES} {MASTER} -> {TARGET_IP}\")\n",
    "plt.xlabel(f\"Time ({TIMERESL})\")\n",
    "plt.ylabel(\"Data, KB\")\n",
    "plt.plot(x, ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fccb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"timestamp\": x, \"size\": y})\n",
    "df.to_csv('alexnet-200-tf.csv', index=False)\n",
    "# ts = pd.read_csv('gpt-200-pytorch.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af42da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    mean = df[c].mean()\n",
    "    stdev = df[c].std()\n",
    "\n",
    "    df[c] = (df[c] - mean) / stdev\n",
    "    df[c] = (df[c] - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461421d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nts = df[\\'timestamp\\'].values\\nvals = df[\\'size\\'].values\\ny = np.empty(0)\\nx = np.empty(0)\\ntimes = np.arange(ts[0], ts[-1], RES)\\ntimes = np.column_stack((times[:-1], times[1:]))\\nfw = 100\\nwhile True:\\n    tmp_times = times[:fw]\\n    times = times[fw:]\\n    a = np.logical_and(tmp_times[:, 0][:, None] < ts, ts < tmp_times[:, 1][:, None])\\n    x = np.append(x, (a * vals).sum(axis=1))\\n    y = np.append(y, tmp_times[:, 0])\\n    if len(times) < fw:\\n        break\\nplt.figure(figsize=(50, 10))\\nplt.title(f\"Res: {RES} {TIMERES} {MASTER} -> {TARGET_IP}\")\\nplt.xlabel(f\"Time ({TIMERESL})\")\\nplt.ylabel(\"Data, KB\")\\nplt.plot(x, y)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ts = df['timestamp'].values\n",
    "vals = df['size'].values\n",
    "y = np.empty(0)\n",
    "x = np.empty(0)\n",
    "times = np.arange(ts[0], ts[-1], RES)\n",
    "times = np.column_stack((times[:-1], times[1:]))\n",
    "fw = 100\n",
    "while True:\n",
    "    tmp_times = times[:fw]\n",
    "    times = times[fw:]\n",
    "    a = np.logical_and(tmp_times[:, 0][:, None] < ts, ts < tmp_times[:, 1][:, None])\n",
    "    x = np.append(x, (a * vals).sum(axis=1))\n",
    "    y = np.append(y, tmp_times[:, 0])\n",
    "    if len(times) < fw:\n",
    "        break\n",
    "plt.figure(figsize=(50, 10))\n",
    "plt.title(f\"Res: {RES} {TIMERES} {MASTER} -> {TARGET_IP}\")\n",
    "plt.xlabel(f\"Time ({TIMERESL})\")\n",
    "plt.ylabel(\"Data, KB\")\n",
    "plt.plot(x, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a535f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, DMatrix, train\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score,\n",
    "    mean_absolute_error, accuracy_score,\n",
    "    f1_score, roc_auc_score,\n",
    "    log_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be6215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_files(df, window_size, target_column='size'):\n",
    "    target = df[target_column]\n",
    "    final_df = df.copy()\n",
    "    for sample_num in range(1, window_size):\n",
    "        shifted = df.shift(sample_num)\n",
    "        shifted.columns = map(lambda x: x+str(sample_num), shifted.columns)\n",
    "        final_df = pd.concat([shifted, final_df], axis=1)\n",
    "    final_df = final_df.fillna(0)\n",
    "    final_df = final_df.drop(target_column, axis=1)\n",
    "    return final_df, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bfc63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(real, prediction):\n",
    "    mse = mean_squared_error(real, prediction)\n",
    "    mae = mean_absolute_error(real, prediction)\n",
    "    r2 = r2_score(real, prediction)\n",
    "\n",
    "    return {'mse': mse, 'mae': mae, 'r2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25b7fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, xtest, ytest):\n",
    "    y_pred = model.predict(xtest)\n",
    "    return eval_metrics(ytest, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5dda93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_learn(xtrain, ytrain, xtest, ytest):\n",
    "    model = XGBRegressor(objective='reg:squarederror', \n",
    "                         n_estimators=1000, \n",
    "                         max_depth=10, \n",
    "                         eval_metric=r2_score, \n",
    "                         booster='gbtree',\n",
    "                         predictor=\"gpu_predictor\",\n",
    "                         tree_method='gpu_hist',\n",
    "                        )\n",
    "    model.fit(xtrain, ytrain)\n",
    "    result = {}\n",
    "    result['train'] = eval_model(model, xtrain, ytrain)\n",
    "    result['test'] = eval_model(model, xtest, ytest)\n",
    "\n",
    "    return result, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bebc33d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train': {'mse': 60.79750838143415,\n",
       "   'mae': 1.4129572290306474,\n",
       "   'r2': 0.31010228808286733},\n",
       "  'test': {'mse': 72.8722138408932,\n",
       "   'mae': 1.7878457822723037,\n",
       "   'r2': 0.013655265500170821}},\n",
       " XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=<function r2_score at 0x7fb4d9af0670>, gamma=0,\n",
       "              gpu_id=0, grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012, max_bin=256,\n",
       "              max_cat_to_onehot=4, max_delta_step=0, max_depth=10, max_leaves=0,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=1000, n_jobs=0, num_parallel_tree=1,\n",
       "              predictor='gpu_predictor', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, ...))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs = prepare_files(df, 50)\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, shuffle=False)\n",
    "xgboost_learn(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb6c52",
   "metadata": {},
   "source": [
    "RES = 2000\n",
    "({'train': {'mse': 2213857.391212464,\n",
    "   'mae': 730.6404319857144,\n",
    "   'r2': 0.7279064089400382},\n",
    "  'test': {'mse': 2693812.8702931516,\n",
    "   'mae': 815.724213257461,\n",
    "   'r2': 0.6574755396638999}},"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896d0a3",
   "metadata": {},
   "source": [
    "RES = 200 ResNet NCCL\n",
    "'train': {'mse': 9.101269742950386e-07,\n",
    "  \n",
    "({'train': {'mse': 2.9009046081819423e-07,\n",
    "   'mae': 0.00011134981196798435,\n",
    "   'r2': 0.7098828104436443},\n",
    "  'test': {'mse': 3.43119514130418e-07,\n",
    "   'mae': 0.00012053085141676116,\n",
    "   'r2': 0.6566614615364892}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653b603",
   "metadata": {},
   "source": [
    "RES = 200 GPT C20D\n",
    "({'train': {'mse': 3.222166312277829e-07,\n",
    "   'mae': 0.00012348057231785167,\n",
    "   'r2': 0.6816371602112872},\n",
    "  'test': {'mse': 3.7919567986376153e-07,\n",
    "   'mae': 0.0001340097186057974,\n",
    "   'r2': 0.6124819743566556}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d855b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "target = 'target'\n",
    "features = ['timestamp', 'size']\n",
    "ts = pd.read_csv('alexnet-200-tf.csv')\n",
    "ts = ts.apply(pd.to_numeric)\n",
    "ts[target] = ts['size'].shift(-1)\n",
    "ts = ts[:-1]\n",
    "split_index = int(ts.shape[0] * split)\n",
    "df_train = ts[:-split_index]\n",
    "df_test = ts[-split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5784361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100603/362411511.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[c] = (df_train[c] - mean) / stdev\n",
      "/tmp/ipykernel_100603/362411511.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[c] = (df_test[c] - mean) / stdev\n"
     ]
    }
   ],
   "source": [
    "target_mean = df_train[target].mean()\n",
    "target_stdev = df_train[target].std()\n",
    "\n",
    "for c in df_train.columns:\n",
    "    mean = df_train[c].mean()\n",
    "    stdev = df_train[c].std()\n",
    "\n",
    "    df_train[c] = (df_train[c] - mean) / stdev\n",
    "    df_test[c] = (df_test[c] - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9be4da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length=5):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[target].values).float()\n",
    "        self.X = torch.tensor(dataframe[features].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0:(i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f33909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 50, 2])\n",
      "tensor([[[ 0.5489, -0.1164],\n",
      "         [ 0.5489, -0.1164],\n",
      "         [ 0.5489, -0.1164],\n",
      "         [ 0.5489, -0.1164],\n",
      "         [ 0.5489, -0.1164],\n",
      "         [ 0.5490, -0.1164],\n",
      "         [ 0.5490, -0.1164],\n",
      "         [ 0.5490, -0.1164],\n",
      "         [ 0.5490, -0.1164],\n",
      "         [ 0.5491, -0.1164],\n",
      "         [ 0.5491, -0.1164],\n",
      "         [ 0.5491, -0.1164],\n",
      "         [ 0.5491, -0.1164],\n",
      "         [ 0.5491, -0.1164],\n",
      "         [ 0.5492, -0.1164],\n",
      "         [ 0.5492, -0.1164],\n",
      "         [ 0.5492, -0.1164],\n",
      "         [ 0.5492, -0.1164],\n",
      "         [ 0.5492, -0.1164],\n",
      "         [ 0.5493, -0.1164],\n",
      "         [ 0.5493, -0.1164],\n",
      "         [ 0.5493, -0.1164],\n",
      "         [ 0.5493,  2.2523],\n",
      "         [ 0.5494, -0.1164],\n",
      "         [ 0.5494, -0.1164],\n",
      "         [ 0.5494, -0.1164],\n",
      "         [ 0.5494, -0.1164],\n",
      "         [ 0.5494, -0.1164],\n",
      "         [ 0.5495, -0.1164],\n",
      "         [ 0.5495, -0.1164],\n",
      "         [ 0.5495, -0.1164],\n",
      "         [ 0.5495, -0.1164],\n",
      "         [ 0.5496, -0.1164],\n",
      "         [ 0.5496, -0.1164],\n",
      "         [ 0.5496, -0.1164],\n",
      "         [ 0.5496, -0.1164],\n",
      "         [ 0.5496, -0.1164],\n",
      "         [ 0.5497, -0.1164],\n",
      "         [ 0.5497, -0.1164],\n",
      "         [ 0.5497, -0.1164],\n",
      "         [ 0.5497, -0.1164],\n",
      "         [ 0.5497, -0.1164],\n",
      "         [ 0.5498, -0.1164],\n",
      "         [ 0.5498, -0.1164],\n",
      "         [ 0.5498, -0.1164],\n",
      "         [ 0.5498, -0.1164],\n",
      "         [ 0.5499, -0.1164],\n",
      "         [ 0.5499, -0.1164],\n",
      "         [ 0.5499, -0.1164],\n",
      "         [ 0.5499, -0.1164]],\n",
      "\n",
      "        [[-0.7571, -0.1164],\n",
      "         [-0.7570, -0.1164],\n",
      "         [-0.7570, -0.1164],\n",
      "         [-0.7570, -0.1164],\n",
      "         [-0.7570, -0.1164],\n",
      "         [-0.7569, -0.1164],\n",
      "         [-0.7569, -0.1164],\n",
      "         [-0.7569, -0.1164],\n",
      "         [-0.7569, -0.1164],\n",
      "         [-0.7569, -0.1164],\n",
      "         [-0.7568, -0.1164],\n",
      "         [-0.7568, -0.1164],\n",
      "         [-0.7568, -0.1164],\n",
      "         [-0.7568, -0.1164],\n",
      "         [-0.7568, -0.1164],\n",
      "         [-0.7567, -0.1164],\n",
      "         [-0.7567, -0.1164],\n",
      "         [-0.7567, -0.1164],\n",
      "         [-0.7567, -0.1164],\n",
      "         [-0.7566, -0.1164],\n",
      "         [-0.7566, -0.1164],\n",
      "         [-0.7566, -0.1164],\n",
      "         [-0.7566, -0.1164],\n",
      "         [-0.7566, -0.1164],\n",
      "         [-0.7565, -0.1164],\n",
      "         [-0.7565, -0.1164],\n",
      "         [-0.7565, -0.1164],\n",
      "         [-0.7565, -0.1164],\n",
      "         [-0.7564, -0.1164],\n",
      "         [-0.7564, -0.1164],\n",
      "         [-0.7564, -0.1164],\n",
      "         [-0.7564, -0.1164],\n",
      "         [-0.7564, -0.1164],\n",
      "         [-0.7563, -0.1164],\n",
      "         [-0.7563, -0.1164],\n",
      "         [-0.7563, -0.1164],\n",
      "         [-0.7563, -0.1164],\n",
      "         [-0.7563, -0.1164],\n",
      "         [-0.7562, -0.1164],\n",
      "         [-0.7562, -0.1164],\n",
      "         [-0.7562, -0.1164],\n",
      "         [-0.7562, -0.1164],\n",
      "         [-0.7561, -0.1164],\n",
      "         [-0.7561, -0.1164],\n",
      "         [-0.7561, -0.1164],\n",
      "         [-0.7561, -0.1164],\n",
      "         [-0.7561, -0.1164],\n",
      "         [-0.7560, -0.1164],\n",
      "         [-0.7560, -0.1164],\n",
      "         [-0.7560, -0.1164]],\n",
      "\n",
      "        [[ 1.2140, -0.1164],\n",
      "         [ 1.2140, -0.1164],\n",
      "         [ 1.2140, -0.1164],\n",
      "         [ 1.2141, -0.1164],\n",
      "         [ 1.2141, -0.1164],\n",
      "         [ 1.2141, -0.1164],\n",
      "         [ 1.2141, -0.1164],\n",
      "         [ 1.2142, -0.1164],\n",
      "         [ 1.2142, -0.1164],\n",
      "         [ 1.2142, -0.1164],\n",
      "         [ 1.2142, -0.1164],\n",
      "         [ 1.2142, -0.1164],\n",
      "         [ 1.2143, -0.1164],\n",
      "         [ 1.2143, -0.1164],\n",
      "         [ 1.2143, -0.1164],\n",
      "         [ 1.2143,  1.5071],\n",
      "         [ 1.2144, -0.1164],\n",
      "         [ 1.2144, -0.1164],\n",
      "         [ 1.2144, -0.1164],\n",
      "         [ 1.2144, -0.1164],\n",
      "         [ 1.2144, -0.1164],\n",
      "         [ 1.2145, -0.1164],\n",
      "         [ 1.2145, -0.1164],\n",
      "         [ 1.2145,  7.6460],\n",
      "         [ 1.2145,  4.4435],\n",
      "         [ 1.2145, -0.1164],\n",
      "         [ 1.2146, -0.1164],\n",
      "         [ 1.2146,  2.1991],\n",
      "         [ 1.2146, -0.1164],\n",
      "         [ 1.2146, -0.1164],\n",
      "         [ 1.2147, -0.1164],\n",
      "         [ 1.2147, -0.1164],\n",
      "         [ 1.2147, -0.1164],\n",
      "         [ 1.2147, -0.1164],\n",
      "         [ 1.2147, -0.1164],\n",
      "         [ 1.2148, -0.1164],\n",
      "         [ 1.2148, -0.1164],\n",
      "         [ 1.2148, -0.1164],\n",
      "         [ 1.2148, -0.1164],\n",
      "         [ 1.2149, -0.1164],\n",
      "         [ 1.2149,  2.8023],\n",
      "         [ 1.2149, -0.1164],\n",
      "         [ 1.2149, -0.1164],\n",
      "         [ 1.2149, -0.1164],\n",
      "         [ 1.2150, -0.1164],\n",
      "         [ 1.2150, -0.1164],\n",
      "         [ 1.2150, -0.1164],\n",
      "         [ 1.2150, -0.1164],\n",
      "         [ 1.2150, -0.1164],\n",
      "         [ 1.2151, -0.1164]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(99)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47c222e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([128, 128, 2])\n",
      "Target shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "batch_size = 128\n",
    "sequence_length = 128\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37d58aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_features, hidden_units):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6807f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_hidden_units = 64\n",
    "\n",
    "model = ShallowRegressionLSTM(num_features=len(features), hidden_units=num_hidden_units)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0167927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    y_pred = []\n",
    "    y_real = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "            y_pred.extend(output.numpy())\n",
    "            y_real.extend(y.numpy())\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "    print(f\"R2 score: {r2_score(y_real, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56009123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "--------\n",
      "Test loss: 0.8405627168801565\n",
      "R2 score: -0.0025698171718053775\n",
      "\n",
      "Epoch 0\n",
      "---------\n",
      "Train loss: 0.9980794819581579\n",
      "Test loss: 0.8350524965536374\n",
      "R2 score: 0.004003376221292654\n",
      "\n",
      "Epoch 1\n",
      "---------\n",
      "Train loss: 0.994266715877576\n",
      "Test loss: 0.8330200204912287\n",
      "R2 score: 0.006428917011861546\n",
      "\n",
      "Epoch 2\n",
      "---------\n",
      "Train loss: 0.9912460904832607\n",
      "Test loss: 0.830978667385912\n",
      "R2 score: 0.008864562717770919\n",
      "\n",
      "Epoch 3\n",
      "---------\n",
      "Train loss: 0.9880021697698305\n",
      "Test loss: 0.8294705074229756\n",
      "R2 score: 0.01066398184083761\n",
      "\n",
      "Epoch 4\n",
      "---------\n",
      "Train loss: 0.9856897299155557\n",
      "Test loss: 0.8278548217718847\n",
      "R2 score: 0.012591146990837188\n",
      "\n",
      "Epoch 5\n",
      "---------\n",
      "Train loss: 0.9830168916460258\n",
      "Test loss: 0.8268331653581789\n",
      "R2 score: 0.013809372292562516\n",
      "\n",
      "Epoch 6\n",
      "---------\n",
      "Train loss: 0.9820899416891058\n",
      "Test loss: 0.8262300617121255\n",
      "R2 score: 0.014529081930570897\n",
      "\n",
      "Epoch 7\n",
      "---------\n",
      "Train loss: 0.9805930501882895\n",
      "Test loss: 0.8259279870469936\n",
      "R2 score: 0.01488935750589182\n",
      "\n",
      "Epoch 8\n",
      "---------\n",
      "Train loss: 0.9803591164042397\n",
      "Test loss: 0.8256820238610276\n",
      "R2 score: 0.015183135035700346\n",
      "\n",
      "Epoch 9\n",
      "---------\n",
      "Train loss: 0.9804707981104281\n",
      "Test loss: 0.8254740359589983\n",
      "R2 score: 0.015431368487368258\n",
      "\n",
      "Epoch 10\n",
      "---------\n",
      "Train loss: 0.9793182826527836\n",
      "Test loss: 0.8254055552668154\n",
      "R2 score: 0.015512893730631805\n",
      "\n",
      "Epoch 11\n",
      "---------\n",
      "Train loss: 0.9794444596381464\n",
      "Test loss: 0.8254286104063981\n",
      "R2 score: 0.015485905816765233\n",
      "\n",
      "Epoch 12\n",
      "---------\n",
      "Train loss: 0.9758145893820955\n",
      "Test loss: 0.8228016299768709\n",
      "R2 score: 0.01861924230928269\n",
      "\n",
      "Epoch 13\n",
      "---------\n",
      "Train loss: 0.9718432485272225\n",
      "Test loss: 0.8204757333628666\n",
      "R2 score: 0.021393805816712907\n",
      "\n",
      "Epoch 14\n",
      "---------\n",
      "Train loss: 0.9718481063781581\n",
      "Test loss: 0.8199960273833802\n",
      "R2 score: 0.02196489172807281\n",
      "\n",
      "Epoch 15\n",
      "---------\n",
      "Train loss: 0.9703728501186467\n",
      "Test loss: 0.8209739604887242\n",
      "R2 score: 0.020798504356869918\n",
      "\n",
      "Epoch 16\n",
      "---------\n",
      "Train loss: 0.9701657408060362\n",
      "Test loss: 0.8196600633180844\n",
      "R2 score: 0.02236609622668584\n",
      "\n",
      "Epoch 17\n",
      "---------\n",
      "Train loss: 0.9707484597590967\n",
      "Test loss: 0.822750146417186\n",
      "R2 score: 0.018681299035028687\n",
      "\n",
      "Epoch 18\n",
      "---------\n",
      "Train loss: 0.9703996101833092\n",
      "Test loss: 0.8215790545173849\n",
      "R2 score: 0.020077228301341732\n",
      "\n",
      "Epoch 19\n",
      "---------\n",
      "Train loss: 0.9699197035545531\n",
      "Test loss: 0.8183800760523763\n",
      "R2 score: 0.023895192994793923\n",
      "\n",
      "Epoch 20\n",
      "---------\n",
      "Train loss: 0.96958824494579\n",
      "Test loss: 0.8219719912606076\n",
      "R2 score: 0.019609138129407788\n",
      "\n",
      "Epoch 21\n",
      "---------\n",
      "Train loss: 0.97001476305722\n",
      "Test loss: 0.8210362841178162\n",
      "R2 score: 0.020724978772447167\n",
      "\n",
      "Epoch 22\n",
      "---------\n",
      "Train loss: 0.9695320063746737\n",
      "Test loss: 0.8194685713970102\n",
      "R2 score: 0.022595483754070278\n",
      "\n",
      "Epoch 23\n",
      "---------\n",
      "Train loss: 0.9691895226625582\n",
      "Test loss: 0.8188042789123033\n",
      "R2 score: 0.023387867605399126\n",
      "\n",
      "Epoch 24\n",
      "---------\n",
      "Train loss: 0.9687916505037066\n",
      "Test loss: 0.8176072711880223\n",
      "R2 score: 0.024815417600349\n",
      "\n",
      "Epoch 25\n",
      "---------\n",
      "Train loss: 0.9680505763158763\n",
      "Test loss: 0.8179455813787615\n",
      "R2 score: 0.02441177999171118\n",
      "\n",
      "Epoch 26\n",
      "---------\n",
      "Train loss: 0.9681798545897413\n",
      "Test loss: 0.81973098211874\n",
      "R2 score: 0.022282612845745287\n",
      "\n",
      "Epoch 27\n",
      "---------\n",
      "Train loss: 0.9677008066627866\n",
      "Test loss: 0.817832650513036\n",
      "R2 score: 0.024547132698066076\n",
      "\n",
      "Epoch 28\n",
      "---------\n",
      "Train loss: 0.9676909607115727\n",
      "Test loss: 0.8179593889821822\n",
      "R2 score: 0.024396125648966094\n",
      "\n",
      "Epoch 29\n",
      "---------\n",
      "Train loss: 0.967245152057447\n",
      "Test loss: 0.8172606081412269\n",
      "R2 score: 0.025229253915014627\n",
      "\n",
      "Epoch 30\n",
      "---------\n",
      "Train loss: 0.96706115193471\n",
      "Test loss: 0.8171142323341113\n",
      "R2 score: 0.025403731363837\n",
      "\n",
      "Epoch 31\n",
      "---------\n",
      "Train loss: 0.9671425637803995\n",
      "Test loss: 0.8268008177297023\n",
      "R2 score: 0.013848873275743245\n",
      "\n",
      "Epoch 32\n",
      "---------\n",
      "Train loss: 0.9676245916119884\n",
      "Test loss: 0.8170057549559249\n",
      "R2 score: 0.02553508384306813\n",
      "\n",
      "Epoch 33\n",
      "---------\n",
      "Train loss: 0.9688197365687348\n",
      "Test loss: 0.8264508598173658\n",
      "R2 score: 0.014265446182127994\n",
      "\n",
      "Epoch 34\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Untrained test\\n--------\")\n",
    "test_model(test_loader, model, loss_function)\n",
    "print()\n",
    "\n",
    "for ix_epoch in range(50):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    test_model(test_loader, model, loss_function)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a731a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_loader, model, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eec088",
   "metadata": {},
   "source": [
    "RES = 200 ResNet NCCL\n",
    "Test loss: 0.6859737798425152\n",
    "R2 score: 0.3139429240444592\n",
    "\n",
    "RES = 200 GPT C20D\n",
    "Test loss: 0.3235129721796335\n",
    "R2 score: 0.6663728481527116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3276b806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9124033429664923"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df['size'] == 0) * 1).sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b93fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
