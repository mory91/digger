{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from tsai.all import *\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from trace_process import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 20_000\n",
    "TEST_SIZE = 5_000\n",
    "TIME_DELTA = 2000 * NANO_TO_MICRO\n",
    "BATCH_SIZE = 32\n",
    "MTU = 1514\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "TRAIN_PATH = \"../data/15/node-2/train/packets\"\n",
    "TEST_PATH = \"../data/15/node-2/test/packets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_packets(file_path):\n",
    "    df = pd.read_csv(\n",
    "        file_path, \n",
    "        header=None,\n",
    "        index_col=False,\n",
    "        names=['timestamp', 'size', 'src', 'dest', 'dir'], \n",
    "        dtype={'size': \"int16\", 'src': \"category\", 'dest': \"category\", \"timestamp\": \"int64\", \"size\": \"int64\", \"dir\": \"int8\"},\n",
    "    )\n",
    "    df = df[df['dir'] == 1][[\"timestamp\", \"size\"]]\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, test=False):\n",
    "        raw_vals = get_packets(path)\n",
    "        self.is_testing = test\n",
    "        vals, times, sizes, _ = get_flow_trace_time(raw_vals, TIME_DELTA)\n",
    "        sizes = np.roll(sizes, -1)\n",
    "        self.vals = vals\n",
    "        self.times = times\n",
    "        self.sizes = sizes\n",
    "        self.threshold = np.median(sizes)\n",
    "        self.max_len = np.max([len(f) for f in self.times])\n",
    "        self.labels = torch.Tensor((sizes > self.threshold) * 1)\n",
    "    def __len__(self):\n",
    "        if self.is_testing:\n",
    "            return 1000\n",
    "        else:\n",
    "            return 10000\n",
    "            #return len(self.times)\n",
    "    def __getitem__(self, idx):\n",
    "        t1 = torch.Tensor(self.vals[idx])\n",
    "        t2 = torch.Tensor(self.times[idx])\n",
    "        t1 = F.pad(t1, (self.max_len - t1.shape[0], 0))\n",
    "        t2 = F.pad(t2, (self.max_len - t2.shape[0], 0))\n",
    "        return torch.cat((t1.unsqueeze(1), t2.unsqueeze(1)), dim=1), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(TraceDataset(TRAIN_PATH), batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
    "test_data = DataLoader(TraceDataset(TEST_PATH, test=True), batch_size=BATCH_SIZE, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_units, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_units, batch_first=True, num_layers=self.num_layers)\n",
    "        self.linear = torch.nn.Linear(in_features=self.hidden_units, out_features=output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_units).cuda()\n",
    "        c0 = torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_units).cuda()\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0)) # preserve states?\n",
    "        out = self.linear(output[:, -1]).flatten()\n",
    "\n",
    "        return F.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 312/312 [01:59<00:00,  2.61it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 948.00 MiB (GPU 0; 11.75 GiB total capacity; 8.14 GiB already allocated; 170.69 MiB free; 9.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/morteza/code/digger/morteza/lstm_packet.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab-morteza/home/morteza/code/digger/morteza/lstm_packet.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m x_val \u001b[39m=\u001b[39m x_val\u001b[39m.\u001b[39mpermute((\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab-morteza/home/morteza/code/digger/morteza/lstm_packet.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m x_val, y_val \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mcuda() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m (x_val, y_val)]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blab-morteza/home/morteza/code/digger/morteza/lstm_packet.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m out \u001b[39m=\u001b[39m model(x_val)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab-morteza/home/morteza/code/digger/morteza/lstm_packet.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blab-morteza/home/morteza/code/digger/morteza/lstm_packet.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/tsai/models/RNN_FCN.py:47\u001b[0m, in \u001b[0;36m_RNN_FCN_Base.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvblock1(x)\n\u001b[1;32m     46\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mse1(x)\n\u001b[0;32m---> 47\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvblock2(x)\n\u001b[1;32m     48\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mse2(x)\n\u001b[1;32m     49\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvblock3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/functional.py:2406\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2400\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies Batch Normalization for each channel across a batch of data.\u001b[39;00m\n\u001b[1;32m   2401\u001b[0m \n\u001b[1;32m   2402\u001b[0m \u001b[39mSee :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,\u001b[39;00m\n\u001b[1;32m   2403\u001b[0m \u001b[39m:class:`~torch.nn.BatchNorm3d` for details.\u001b[39;00m\n\u001b[1;32m   2404\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2405\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias):\n\u001b[0;32m-> 2406\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2407\u001b[0m         batch_norm,\n\u001b[1;32m   2408\u001b[0m         (\u001b[39minput\u001b[39;49m, running_mean, running_var, weight, bias),\n\u001b[1;32m   2409\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2410\u001b[0m         running_mean,\n\u001b[1;32m   2411\u001b[0m         running_var,\n\u001b[1;32m   2412\u001b[0m         weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m   2413\u001b[0m         bias\u001b[39m=\u001b[39;49mbias,\n\u001b[1;32m   2414\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   2415\u001b[0m         momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m   2416\u001b[0m         eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m   2417\u001b[0m     )\n\u001b[1;32m   2418\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2419\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/overrides.py:1394\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1389\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mwill be an error in PyTorch 1.11, please define it as a classmethod.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1390\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1392\u001b[0m \u001b[39m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[39m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m result \u001b[39m=\u001b[39m torch_func_method(public_api, types, args, kwargs)\n\u001b[1;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1397\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/fastai/torch_core.py:376\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> 376\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    377\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/_tensor.py:1142\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1141\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1142\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1144\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-test/lib/python3.9/site-packages/torch/nn/functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2419\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2421\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2422\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2423\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 948.00 MiB (GPU 0; 11.75 GiB total capacity; 8.14 GiB already allocated; 170.69 MiB free; 9.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "input_dim = 2    \n",
    "hidden_dim = 256\n",
    "layer_dim = 3\n",
    "output_dim = 1 \n",
    "\n",
    "lr = 0.0005\n",
    "n_epochs = 10\n",
    "iterations_per_epoch = len(train_data)\n",
    "best_acc = 0\n",
    "\n",
    "model = MLSTM_FCN(c_in=2, c_out=1, seq_len=5906, shuffle=False) # TraceModel(input_dim, hidden_dim, output_dim, layer_dim)\n",
    "model = model.cuda()\n",
    "criterion = torch.nn.BCELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# sched = torch.optim.CyclicLR(opt, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(tqdm(train_data)):\n",
    "        model.train()\n",
    "        x_batch = x_batch.permute((0, 2, 1))\n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        # sched.step()\n",
    "        opt.zero_grad()\n",
    "        out = model(x_batch)\n",
    "        out = F.sigmoid(out.view(-1))\n",
    "        loss = criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x_val, y_val in test_data:\n",
    "        x_val = x_val.permute((0, 2, 1))\n",
    "        x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "        out = model(x_val)\n",
    "        out = F.sigmoid(out.view(-1))\n",
    "        preds = torch.round(out)\n",
    "        total += y_val.size(0)\n",
    "        correct += (preds == y_val).sum().item()\n",
    "    \n",
    "    acc = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
